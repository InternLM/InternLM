# InternLM

<div align="center">

<img src="./assets//logo.svg" width="200"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="5">ä¹¦ç”ŸÂ·æµ¦è¯­ å®˜ç½‘</font></b>
    <sup>
      <a href="https://internlm.intern-ai.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    <div>&nbsp;</div>
  </div>

[![license](./assets//license.svg)](https://github.com/open-mmlab/mmdetection/blob/main/LICENSE)
[![evaluation](./assets//compass_support.svg)](https://github.com/internLM/OpenCompass/)
<!-- [![Documentation Status](https://readthedocs.org/projects/internlm/badge/?version=latest)](https://internlm.readthedocs.io/zh_CN/latest/?badge=latest) -->

[ğŸ“˜å¯¹è¯æ•™ç¨‹](./chat) |
[ğŸ› ï¸æ™ºèƒ½ä½“æ•™ç¨‹](./agent) |
[ğŸ“Šè¯„æµ‹](./evaluation) |
[ğŸ‘€æ¨¡å‹åº“](./model_cards) |
[ğŸ¤—HuggingFace](https://huggingface.co/spaces/internlm/internlm2-Chat-7B) |
[ğŸ†•Update News](#news) |
[ğŸ¤”æäº¤åé¦ˆ](https://github.com/InternLM/InternLM/issues/new)

[English](./README.md) |
[ç®€ä½“ä¸­æ–‡](./README_zh-CN.md) |

</div>

<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> å’Œ <a href="https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce" target="_blank">å¾®ä¿¡ç¤¾åŒº</a>
</p>

## ç®€ä»‹

InternLM æ˜¯ä¸€ä¸ªå¼€æºçš„è½»é‡çº§è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒè€Œæ— éœ€å¤§é‡çš„ä¾èµ–ã€‚é€šè¿‡å•ä¸€çš„ä»£ç åº“ï¼Œå®ƒæ”¯æŒåœ¨æ‹¥æœ‰æ•°åƒä¸ª GPU çš„å¤§å‹é›†ç¾¤ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ä¼˜åŒ–ã€‚åœ¨1024ä¸ª GPU ä¸Šè®­ç»ƒæ—¶ï¼ŒInternLM å¯ä»¥å®ç°è¿‘90%çš„åŠ é€Ÿæ•ˆç‡ã€‚

åŸºäºInternLMè®­ç»ƒæ¡†æ¶ï¼Œæˆ‘ä»¬å·²ç»å‘å¸ƒäº†ä¸¤ä¸ªå¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ï¼šInternLM-7B å’Œ InternLM-20Bã€‚

## æ›´æ–°

[2024.01.17] æˆ‘ä»¬å‘å¸ƒäº† InternLM2-7B å’Œ InternLM2-20B ä»¥åŠç›¸å…³çš„å¯¹è¯æ¨¡å‹ï¼ŒInternLM2 åœ¨æ•°ç†ã€ä»£ç ã€å¯¹è¯ã€åˆ›ä½œç­‰å„æ–¹é¢èƒ½åŠ›éƒ½è·å¾—äº†é•¿è¶³è¿›æ­¥ï¼Œç»¼åˆæ€§èƒ½è¾¾åˆ°å¼€æºæ¨¡å‹çš„é¢†å…ˆæ°´å¹³ã€‚å¯ä»¥ç‚¹å‡» [ä¸‹é¢çš„æ¨¡å‹åº“](#model-zoo)è¿›è¡Œä¸‹è½½æˆ–è€…[æŸ¥çœ‹æ¨¡å‹æ–‡æ¡£](./model_cards/)æ¥äº†è§£æ›´å¤šç»†èŠ‚.

[2023.12.13] æˆ‘ä»¬æ›´æ–°äº† InternLM-7B-Chat å’Œ InternLM-20B-Chat æ¨¡å‹æƒé‡ã€‚é€šè¿‡æ”¹è¿›å¾®è°ƒæ•°æ®å’Œè®­ç»ƒç­–ç•¥ï¼Œæ–°ç‰ˆå¯¹è¯æ¨¡å‹ç”Ÿæˆçš„å›å¤è´¨é‡æ›´é«˜ã€è¯­è¨€é£æ ¼æ›´åŠ å¤šå…ƒã€‚

[2023.09.20] InternLM-20B å·²å‘å¸ƒï¼ŒåŒ…æ‹¬åŸºç¡€ç‰ˆå’Œå¯¹è¯ç‰ˆã€‚

## Model Zoo

| Model | Transformers(HF) | ModelScope(HF) | OpenXLab(HF) | Release Date |
|---------------------------|------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|
| **InternLM2 Chat 20B**     | [ğŸ¤—internlm/internlm-chat-20b](https://huggingface.co/internlm/internlm2-chat-20b)         | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary)         | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b)     | 2024-01-17   |
| **InternLM2 20B** | [ğŸ¤—internlm/internlm2-20b](https://huggingface.co/internlm/internlm2-20b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b) | 2024-01-17 |
| **InternLM2 Chat 20B SFT**     | [ğŸ¤—internlm/internlm-chat-20b-sft](https://huggingface.co/internlm/internlm2-chat-20b-sft)         | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-20b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary)         | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-sft)     | 2024-01-17   |
| **InternLM2 Base 20B** | [ğŸ¤—internlm/internlm2-base-20b](https://huggingface.co/internlm/internlm2-base-20b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-base-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-20b) | 2024-01-17 |
| **InternLM2 Chat 7B**      | [ğŸ¤—internlm/internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b)           | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary)           | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b)      | 2024-01-17  |
| **InternLM2 7B**           | [ğŸ¤—internlm/internlm2-7b](https://huggingface.co/internlm/internlm2-7b)                     | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary)                     | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b)           |  2024-01-17   |
| **InternLM2 Chat 7B SFT**      | [ğŸ¤—internlm/internlm2-chat-7b-sft](https://huggingface.co/internlm/internlm2-chat-7b-sft)           | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-7b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary)           | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-sft)      | 2024-01-17  |
| **InternLM2 Base 7B**           | [ğŸ¤—internlm/internlm2-base-7b](https://huggingface.co/internlm/internlm2-base-7b)                     | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-base-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary)                     | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-7b)           |  2024-01-17   |

## ä½¿ç”¨æ¡ˆä¾‹

æ¥ä¸‹æ¥æˆ‘ä»¬å±•ç¤ºä½¿ç”¨ [Transformers](#import-from-transformers), [ModelScope](#import-from-modelscope), å’Œ [Web demo](#dialogue) è¿›è¡Œæ¨ç†.
å¯¹è¯æ¨¡å‹é‡‡ç”¨äº† [chatml æ ¼å¼](./chat/chat_format.md) æ¥æ”¯æŒé€šç”¨å¯¹è¯å’Œæ™ºèƒ½ä½“åº”ç”¨ã€‚

### é€šè¿‡ Transformers åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» Transformers åŠ è½½ InternLM æ¨¡å‹ ï¼ˆå¯ä¿®æ”¹æ¨¡å‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡å‹ï¼‰

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-chat-7b", trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("internlm/internlm2-chat-7b", trust_remote_code=True).cuda()
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
>>> response, history = model.chat(tokenizer, "è¯·æä¾›ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ã€‚", history=history)
>>> print(response)
```

### é€šè¿‡ ModelScope åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» ModelScope åŠ è½½ InternLM æ¨¡å‹ ï¼ˆå¯ä¿®æ”¹æ¨¡å‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡å‹ï¼‰

```python
from modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM
import torch
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-7b')
tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", trust_remote_code=True,torch_dtype=torch.float16)
model = AutoModelForCausalLM.from_pretrained(model_dir,device_map="auto",  trust_remote_code=True,torch_dtype=torch.float16)
model = model.eval()
response, history = model.chat(tokenizer, "hello", history=[])
print(response)
response, history = model.chat(tokenizer, "please provide three suggestions about time management", history=history)
print(response)
```

### é€šè¿‡å‰ç«¯ç½‘é¡µå¯¹è¯

å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å¯åŠ¨ä¸€ä¸ªå‰ç«¯çš„ç•Œé¢æ¥ä¸ InternLM Chat 7B æ¨¡å‹è¿›è¡Œäº¤äº’

```bash
pip install streamlit==1.24.0
pip install transformers==4.30.2
streamlit run ./chat/web_demo.py
```

æ•ˆæœå¦‚ä¸‹

![æ•ˆæœ](https://github.com/InternLM/InternLM/assets/9102141/11b60ee0-47e4-42c0-8278-3051b2f17fe4)

### åŸºäºInternLMé«˜æ€§èƒ½éƒ¨ç½²

æˆ‘ä»¬ä½¿ç”¨ [LMDeploy](https://github.com/InternLM/LMDeploy) å®Œæˆ InternLM çš„ä¸€é”®éƒ¨ç½²ã€‚

```shell
python3 -m pip install lmdeploy
lmdeploy chat turbomind InternLM/internlm-chat-7b --model-name internlm-chat-7b
```

è¯·å‚è€ƒ[éƒ¨ç½²æŒ‡å—](./chat/lmdeploy.md)äº†è§£æ›´å¤šä½¿ç”¨æ¡ˆä¾‹ï¼Œæ›´å¤šéƒ¨ç½²æ•™ç¨‹åˆ™å¯åœ¨[è¿™é‡Œ](https://github.com/InternLM/LMDeploy)æ‰¾åˆ°ã€‚

## å¾®è°ƒ&è®­ç»ƒ

è¯·å‚è€ƒ[å¾®è°ƒæ•™ç¨‹](./finetune/)å°è¯•ç»­è®­æˆ–å¾®è°ƒ InternLM2ã€‚

**æ³¨æ„ï¼š**æœ¬é¡¹ç›®ä¸­çš„å…¨é‡è®­ç»ƒåŠŸèƒ½å·²ç»è¿ç§»åˆ°äº†[InternEvo](https://github.com/InternLM/InternEvo)ä»¥ä¾¿æ·ç”¨æˆ·çš„ä½¿ç”¨ã€‚InternEvo æä¾›äº†é«˜æ•ˆçš„é¢„è®­ç»ƒå’Œå¾®è°ƒåŸºå»ºç”¨äºè®­ç»ƒ InternLM ç³»åˆ—æ¨¡å‹ã€‚

## è´¡çŒ®

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ InternLM æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚éå¸¸æ¬¢è¿ç¤¾åŒºç”¨æˆ·èƒ½å‚ä¸è¿›é¡¹ç›®ä¸­æ¥ã€‚è¯·å‚è€ƒè´¡çŒ®æŒ‡å—æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

InternLM ä»£ç åº“æ˜¯ä¸€æ¬¾ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å’Œæ¥è‡ªä¸åŒé«˜æ ¡ã€ä¼ä¸šçš„ç ”å‘äººå‘˜å…±åŒå‚ä¸è´¡çŒ®çš„å¼€æºé¡¹ç›®ã€‚æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®æä¾›æ–°åŠŸèƒ½æ”¯æŒçš„è´¡çŒ®è€…ï¼Œä»¥åŠæä¾›å®è´µåé¦ˆçš„ç”¨æˆ·ã€‚ æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå·¥å…·ç®±å’ŒåŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºç¤¾åŒºæä¾›çµæ´»é«˜æ•ˆçš„ä»£ç å·¥å…·ï¼Œä¾›ç”¨æˆ·å¾®è°ƒ InternLM å¹¶å¼€å‘è‡ªå·±çš„æ–°æ¨¡å‹ï¼Œä»è€Œä¸æ–­ä¸ºå¼€æºç¤¾åŒºæä¾›è´¡çŒ®ã€‚ç‰¹åˆ«é¸£è°¢[flash-attention](https://github.com/HazyResearch/flash-attention) ä¸ [ColossalAI](https://github.com/hpcaitech/ColossalAI) ä¸¤é¡¹å¼€æºé¡¹ç›®ã€‚

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚

## å¼•ç”¨

```
@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}
```
