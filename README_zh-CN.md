# InternLM

<div align="center">

<img src="./assets//logo.svg" width="200"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="5">ä¹¦ç”ŸÂ·æµ¦è¯­ å®˜ç½‘</font></b>
    <sup>
      <a href="https://internlm.intern-ai.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    <div>&nbsp;</div>
  </div>

[![license](./assets//license.svg)](https://github.com/open-mmlab/mmdetection/blob/main/LICENSE)
[![evaluation](./assets//compass_support.svg)](https://github.com/internLM/OpenCompass/)

<!-- [![Documentation Status](https://readthedocs.org/projects/internlm/badge/?version=latest)](https://internlm.readthedocs.io/zh_CN/latest/?badge=latest) -->

[ğŸ“˜å•†ä¸šæˆæƒ](#å¼€æºè®¸å¯è¯) |
[ğŸ¤—HuggingFace](https://huggingface.co/internlm) |
[ğŸ†•æœ€æ–°æ¶ˆæ¯](#æ›´æ–°) |
[ğŸ¤”æäº¤åé¦ˆ](https://github.com/InternLM/InternLM/issues/new)

[English](./README.md) |
[ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)

</div>

<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> å’Œ <a href="https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce" target="_blank">å¾®ä¿¡ç¤¾åŒº</a>
</p>

## ç®€ä»‹

InternLM2 ç³»åˆ—æ¨¡å‹åœ¨æœ¬ä»“åº“æ­£å¼å‘å¸ƒï¼Œå…·æœ‰å¦‚ä¸‹ç‰¹æ€§ï¼š

- æœ‰æ•ˆæ”¯æŒ20ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡ï¼šæ¨¡å‹åœ¨ 20 ä¸‡å­—é•¿è¾“å…¥ä¸­å‡ ä¹å®Œç¾åœ°å®ç°é•¿æ–‡â€œå¤§æµ·æé’ˆâ€ï¼Œè€Œä¸”åœ¨ LongBench å’Œ L-Eval ç­‰é•¿æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿè¾¾åˆ°å¼€æºæ¨¡å‹ä¸­çš„é¢†å…ˆæ°´å¹³ã€‚ å¯ä»¥é€šè¿‡ [LMDeploy](./chat/lmdeploy_zh_cn.md) å°è¯•20ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚
- ç»¼åˆæ€§èƒ½å…¨é¢æå‡ï¼šå„èƒ½åŠ›ç»´åº¦ç›¸æ¯”ä¸Šä¸€ä»£æ¨¡å‹å…¨é¢è¿›æ­¥ï¼Œåœ¨æ¨ç†ã€æ•°å­¦ã€ä»£ç ã€å¯¹è¯ä½“éªŒã€æŒ‡ä»¤éµå¾ªå’Œåˆ›æ„å†™ä½œç­‰æ–¹é¢çš„èƒ½åŠ›æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œç»¼åˆæ€§èƒ½è¾¾åˆ°åŒé‡çº§å¼€æºæ¨¡å‹çš„é¢†å…ˆæ°´å¹³ï¼Œåœ¨é‡ç‚¹èƒ½åŠ›è¯„æµ‹ä¸Š InternLM2-Chat-20B èƒ½æ¯”è‚©ç”šè‡³è¶…è¶Š ChatGPT ï¼ˆGPT-3.5ï¼‰ã€‚
- ä»£ç è§£é‡Šå™¨ä¸æ•°æ®åˆ†æï¼šåœ¨é…åˆä»£ç è§£é‡Šå™¨ï¼ˆcode-interpreterï¼‰çš„æ¡ä»¶ä¸‹ï¼ŒInternLM2-Chat-20B åœ¨ GSM8K å’Œ MATH ä¸Šå¯ä»¥è¾¾åˆ°å’Œ GPT-4 ç›¸ä»¿çš„æ°´å¹³ã€‚åŸºäºåœ¨æ•°ç†å’Œå·¥å…·æ–¹é¢å¼ºå¤§çš„åŸºç¡€èƒ½åŠ›ï¼ŒInternLM2-Chat æä¾›äº†å®ç”¨çš„æ•°æ®åˆ†æèƒ½åŠ›ã€‚
- å·¥å…·è°ƒç”¨èƒ½åŠ›æ•´ä½“å‡çº§ï¼šåŸºäºæ›´å¼ºå’Œæ›´å…·æœ‰æ³›åŒ–æ€§çš„æŒ‡ä»¤ç†è§£ã€å·¥å…·ç­›é€‰ä¸ç»“æœåæ€ç­‰èƒ½åŠ›ï¼Œæ–°ç‰ˆæ¨¡å‹å¯ä»¥æ›´å¯é åœ°æ”¯æŒå¤æ‚æ™ºèƒ½ä½“çš„æ­å»ºï¼Œæ”¯æŒå¯¹å·¥å…·è¿›è¡Œæœ‰æ•ˆçš„å¤šè½®è°ƒç”¨ï¼Œå®Œæˆè¾ƒå¤æ‚çš„ä»»åŠ¡ã€‚å¯ä»¥æŸ¥çœ‹æ›´å¤š[æ ·ä¾‹](./agent/)ã€‚

## æ›´æ–°

\[2024.01.23\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2-Math-7B å’Œ InternLM2-Math-20B ä»¥åŠç›¸å…³çš„å¯¹è¯æ¨¡å‹ã€‚InternLM-Mathä»¥è¾ƒå°çš„å°ºå¯¸è¶…è¿‡äº†ChatGPTçš„è¡¨ç°ã€‚å¯ä»¥ç‚¹å‡»[InternLM-Math](https://github.com/InternLM/internlm-math)è¿›è¡Œä¸‹è½½ï¼Œå¹¶äº†è§£è¯¦æƒ…ã€‚

\[2024.01.17\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2-7B å’Œ InternLM2-20B ä»¥åŠç›¸å…³çš„å¯¹è¯æ¨¡å‹ï¼ŒInternLM2 åœ¨æ•°ç†ã€ä»£ç ã€å¯¹è¯ã€åˆ›ä½œç­‰å„æ–¹é¢èƒ½åŠ›éƒ½è·å¾—äº†é•¿è¶³è¿›æ­¥ï¼Œç»¼åˆæ€§èƒ½è¾¾åˆ°å¼€æºæ¨¡å‹çš„é¢†å…ˆæ°´å¹³ã€‚å¯ä»¥ç‚¹å‡»[ä¸‹é¢çš„æ¨¡å‹åº“](#model-zoo)è¿›è¡Œä¸‹è½½æˆ–è€…[æŸ¥çœ‹æ¨¡å‹æ–‡æ¡£](./model_cards/)æ¥äº†è§£æ›´å¤šç»†èŠ‚.

\[2023.12.13\] æˆ‘ä»¬æ›´æ–°äº† InternLM-7B-Chat å’Œ InternLM-20B-Chat æ¨¡å‹æƒé‡ã€‚é€šè¿‡æ”¹è¿›å¾®è°ƒæ•°æ®å’Œè®­ç»ƒç­–ç•¥ï¼Œæ–°ç‰ˆå¯¹è¯æ¨¡å‹ç”Ÿæˆçš„å›å¤è´¨é‡æ›´é«˜ã€è¯­è¨€é£æ ¼æ›´åŠ å¤šå…ƒã€‚

\[2023.09.20\] InternLM-20B å·²å‘å¸ƒï¼ŒåŒ…æ‹¬åŸºç¡€ç‰ˆå’Œå¯¹è¯ç‰ˆã€‚

## Model Zoo

| Model                      | Transformers(HF)                           | ModelScope(HF)                           | OpenXLab(HF)                           | OpenXLab(Origin)                           | Release Date |
| -------------------------- | ------------------------------------------ | ---------------------------------------- | -------------------------------------- | ------------------------------------------ | ------------ |
| **InternLM2-Base-7B**      | [ğŸ¤—internlm2-base-7b](https://huggingface.co/internlm/internlm2-base-7b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-base-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-7b-original) | 2024-01-17   |
| **InternLM2-7B**           | [ğŸ¤—internlm2-7b](https://huggingface.co/internlm/internlm2-7b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b-original) | 2024-01-17   |
| **InternLM2-Chat-7B-SFT**  | [ğŸ¤—internlm2-chat-7b-sft](https://huggingface.co/internlm/internlm2-chat-7b-sft) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-7b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-sft-original) | 2024-01-17   |
| **InternLM2-Chat-7B**      | [ğŸ¤—internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-original) | 2024-01-17   |
| **InternLM2-Base-20B**     | [ğŸ¤—internlm2-base-20b](https://huggingface.co/internlm/internlm2-base-20b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-base-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-20b-original) | 2024-01-17   |
| **InternLM2-20B**          | [ğŸ¤—internlm2-20b](https://huggingface.co/internlm/internlm2-20b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b-original) | 2024-01-17   |
| **InternLM2-Chat-20B-SFT** | [ğŸ¤—internlm2-chat-20b-sft](https://huggingface.co/internlm/internlm2-chat-20b-sft) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-20b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-sft-original) | 2024-01-17   |
| **InternLM2-Chat-20B**     | [ğŸ¤—internlm2-chat-20b](https://huggingface.co/internlm/internlm2-chat-20b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-chat-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-original) | 2024-01-17   |

**æ¨¡å‹è¯´æ˜ï¼š**

åœ¨æ­¤æ¬¡å‘å¸ƒä¸­ï¼ŒInternLM2 åŒ…å«ä¸¤ç§æ¨¡å‹è§„æ ¼ï¼š7B å’Œ 20Bã€‚7B ä¸ºè½»é‡çº§çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä¸€ä¸ªè½»ä¾¿ä½†æ€§èƒ½ä¸ä¿—çš„æ¨¡å‹ï¼Œ20B æ¨¡å‹çš„ç»¼åˆæ€§èƒ½æ›´ä¸ºå¼ºåŠ²ï¼Œå¯ä»¥æœ‰æ•ˆæ”¯æŒæ›´åŠ å¤æ‚çš„å®ç”¨åœºæ™¯ã€‚æ¯ä¸ªè§„æ ¼ä¸åŒæ¨¡å‹å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://internlm.oss-cn-shanghai.aliyuncs.com/series.png)

1. **InternLM2-Base**ï¼šé«˜è´¨é‡å’Œå…·æœ‰å¾ˆå¼ºå¯å¡‘æ€§çš„æ¨¡å‹åŸºåº§ï¼Œæ˜¯æ¨¡å‹è¿›è¡Œæ·±åº¦é¢†åŸŸé€‚é…çš„é«˜è´¨é‡èµ·ç‚¹ã€‚
2. **InternLM2**ï¼šè¿›ä¸€æ­¥åœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ç»“åˆç‰¹å®šé¢†åŸŸçš„å¢å¼ºè¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œåœ¨è¯„æµ‹ä¸­æˆç»©ä¼˜å¼‚ï¼ŒåŒæ—¶ä¿æŒäº†å¾ˆå¥½çš„é€šç”¨è¯­è¨€èƒ½åŠ›ï¼Œæ˜¯æˆ‘ä»¬æ¨èçš„åœ¨å¤§éƒ¨åˆ†åº”ç”¨ä¸­è€ƒè™‘é€‰ç”¨çš„ä¼˜ç§€åŸºåº§ã€‚
3. **InternLM2-Chat-SFT**: åŸºäº InternLM2-Base æ¨¡å‹è¿›è¡Œäº†æœ‰ç›‘ç£å¾®è°ƒï¼Œæ˜¯ InternLM2-Chat æ¨¡å‹çš„ä¸­é—´ç‰ˆæœ¬ã€‚æˆ‘ä»¬å°†å®ƒä»¬å¼€æºä»¥åŠ©åŠ›ç¤¾åŒºåœ¨å¯¹é½æ–¹é¢çš„ç ”ç©¶ã€‚
4. **InternLM2-Chat**: åœ¨ InternLM2-Chat-SFT çš„åŸºç¡€ä¸Šè¿›è¡Œäº† online RLHF ä»¥è¿›ä¸€æ­¥å¯¹é½. InternLM2-Chat é¢å‘å¯¹è¯äº¤äº’è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå…·æœ‰è¾ƒå¥½çš„æŒ‡ä»¤éµå¾ªã€å…±æƒ…èŠå¤©å’Œè°ƒç”¨å·¥å…·ç­‰çš„èƒ½åŠ›ï¼Œæ˜¯æˆ‘ä»¬æ¨èç›´æ¥ç”¨äºä¸‹æ¸¸åº”ç”¨çš„æ¨¡å‹ã€‚

**å±€é™æ€§ï¼š** å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºæ¨¡å‹å¤§å°ä»¥åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›å¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚

**è¡¥å……è¯´æ˜ï¼š** ä¸Šè¡¨ä¸­çš„ `HF` è¡¨ç¤ºå¯¹åº”æ¨¡å‹ä¸º HuggingFace å¹³å°æä¾›çš„ [transformers](https://github.com/huggingface/transformers) æ¡†æ¶æ ¼å¼ï¼›`Origin` åˆ™è¡¨ç¤ºå¯¹åº”æ¨¡å‹ä¸ºæˆ‘ä»¬ InternLM å›¢é˜Ÿçš„ [InternEvo](https://github.com/InternLM/InternEvo) æ¡†æ¶æ ¼å¼ã€‚

## æ€§èƒ½

### å®¢è§‚è¯„æµ‹

| Dataset          | Baichuan2-7B-Chat | Mistral-7B-Instruct-v0.2 | Qwen-7B-Chat | InternLM2-Chat-7B | ChatGLM3-6B | Baichuan2-13B-Chat | Mixtral-8x7B-Instruct-v0.1 | Qwen-14B-Chat | InternLM2-Chat-20B |
| ---------------- | ----------------- | ------------------------ | ------------ | ----------------- | ----------- | ------------------ | -------------------------- | ------------- | ------------------ |
| MMLU             | 50.1              | 59.2                     | 57.1         | 63.7              | 58.0        | 56.6               | 70.3                       | 66.7          | 66.5               |
| CMMLU            | 53.4              | 42.0                     | 57.9         | 63.0              | 57.8        | 54.8               | 50.6                       | 68.1          | 65.1               |
| AGIEval          | 35.3              | 34.5                     | 39.7         | 47.2              | 44.2        | 40.0               | 41.7                       | 46.5          | 50.3               |
| C-Eval           | 53.9              | 42.4                     | 59.8         | 60.8              | 59.1        | 56.3               | 54.0                       | 71.5          | 63.0               |
| TrivialQA        | 37.6              | 35.0                     | 46.1         | 50.8              | 38.1        | 40.3               | 57.7                       | 54.5          | 53.9               |
| NaturalQuestions | 12.8              | 8.1                      | 18.6         | 24.1              | 14.0        | 12.7               | 22.5                       | 22.9          | 25.9               |
| C3               | 78.5              | 66.9                     | 84.4         | 91.5              | 79.3        | 84.4               | 82.1                       | 91.5          | 93.5               |
| CMRC             | 8.1               | 5.6                      | 14.6         | 63.8              | 43.2        | 27.8               | 5.3                        | 13.0          | 50.4               |
| WinoGrande       | 49.9              | 50.8                     | 54.2         | 65.8              | 61.7        | 50.9               | 60.9                       | 55.7          | 74.8               |
| BBH              | 35.9              | 46.5                     | 45.5         | 61.2              | 56.0        | 42.5               | 57.3                       | 55.8          | 68.3               |
| GSM-8K           | 32.4              | 48.3                     | 44.1         | 70.7              | 53.8        | 56.0               | 71.7                       | 57.7          | 79.6               |
| Math             | 5.7               | 8.6                      | 12.0         | 23.0              | 20.4        | 4.3                | 22.5                       | 27.6          | 31.9               |
| HumanEval        | 17.7              | 35.4                     | 36.0         | 59.8              | 52.4        | 19.5               | 37.8                       | 40.9          | 67.1               |
| MBPP             | 37.7              | 25.7                     | 33.9         | 51.4              | 55.6        | 40.9               | 40.9                       | 30.0          | 65.8               |

- MBPPæ€§èƒ½ä½¿ç”¨çš„æ˜¯MBPP(Sanitized)ç‰ˆæœ¬æ•°æ®é›†

### ä¸»è§‚è¯„æµ‹

- æˆ‘ä»¬è¯„æµ‹äº†InternLM2-Chatåœ¨[AlpacaEval 2.0](https://tatsu-lab.github.io/alpaca_eval/) ä¸Šçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜InternLM2-Chatåœ¨AlpacaEvalä¸Šå·²ç»è¶…è¿‡äº† Claude 2, GPT-4(0613) å’Œ  Gemini Pro.

| Model Name         | Win Rate | Length |
| ------------------ | -------- | ------ |
| GPT-4 Turbo        | 50.00%   | 2049   |
| GPT-4              | 23.58%   | 1365   |
| GPT-4 0314         | 22.07%   | 1371   |
| Mistral Medium     | 21.86%   | 1500   |
| XwinLM 70b V0.1    | 21.81%   | 1775   |
| InternLM2 Chat 20B | 21.75%   | 2373   |
| Mixtral 8x7B v0.1  | 18.26%   | 1465   |
| Claude 2           | 17.19%   | 1069   |
| Gemini Pro         | 16.85%   | 1315   |
| GPT-4 0613         | 15.76%   | 1140   |
| Claude 2.1         | 15.73%   | 1096   |

- æ€§èƒ½æ•°æ®æˆªæ­¢2024-01-17

## ä¾èµ–

- Python >= 3.8
- PyTorch >= 1.12.0 (æ¨è 2.0.0 å’Œæ›´é«˜ç‰ˆæœ¬)
- Transformers >= 4.34

## ä½¿ç”¨æ¡ˆä¾‹

æ¥ä¸‹æ¥æˆ‘ä»¬å±•ç¤ºä½¿ç”¨ [Transformers](#import-from-transformers)ï¼Œ[ModelScope](#import-from-modelscope) å’Œ [Web demo](#dialogue) è¿›è¡Œæ¨ç†ã€‚
å¯¹è¯æ¨¡å‹é‡‡ç”¨äº† [chatml æ ¼å¼](./chat/chat_format.md) æ¥æ”¯æŒé€šç”¨å¯¹è¯å’Œæ™ºèƒ½ä½“åº”ç”¨ã€‚
ä¸ºäº†ä¿éšœæ›´å¥½çš„ä½¿ç”¨æ•ˆæœï¼Œåœ¨ç”¨ [Transformers](#import-from-transformers) æˆ– [ModelScope](#import-from-modelscope) è¿›è¡Œæ¨ç†å‰ï¼Œè¯·ç¡®ä¿å®‰è£…çš„ transformers åº“ç‰ˆæœ¬æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

```
transformers >= 4.34
```

### é€šè¿‡ Transformers åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» Transformers åŠ è½½ InternLM2-7B-Chat æ¨¡å‹ ï¼ˆå¯ä¿®æ”¹æ¨¡å‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡å‹ï¼‰

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-chat-7b", trust_remote_code=True)
# è®¾ç½®`torch_dtype=torch.float16`æ¥å°†æ¨¡å‹ç²¾åº¦æŒ‡å®šä¸ºtorch.float16ï¼Œå¦åˆ™å¯èƒ½ä¼šå› ä¸ºæ‚¨çš„ç¡¬ä»¶åŸå› é€ æˆæ˜¾å­˜ä¸è¶³çš„é—®é¢˜ã€‚
model = AutoModelForCausalLM.from_pretrained("internlm/internlm2-chat-7b", device_map="auto",trust_remote_code=True, torch_dtype=torch.float16)
# (å¯é€‰) å¦‚æœåœ¨ä½èµ„æºè®¾å¤‡ä¸Šï¼Œå¯ä»¥é€šè¿‡bitsandbytesåŠ è½½4-bitæˆ–8-bité‡åŒ–çš„æ¨¡å‹ï¼Œè¿›ä¸€æ­¥èŠ‚çœGPUæ˜¾å­˜.
  # 4-bit é‡åŒ–çš„ InternLM 7B å¤§çº¦ä¼šæ¶ˆè€— 8GB æ˜¾å­˜.
  # pip install -U bitsandbytes
  # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, load_in_8bit=True)
  # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, load_in_4bit=True)
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
# æ¨¡å‹è¾“å‡ºï¼šä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
response, history = model.chat(tokenizer, "è¯·æä¾›ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ã€‚", history=history)
print(response)
```

### é€šè¿‡ ModelScope åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç ä» ModelScope åŠ è½½ InternLM æ¨¡å‹ ï¼ˆå¯ä¿®æ”¹æ¨¡å‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡å‹ï¼‰

```python
import torch
from modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-7b')
tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, torch_dtype=torch.float16)
# (å¯é€‰) å¦‚æœåœ¨ä½èµ„æºè®¾å¤‡ä¸Šï¼Œå¯ä»¥é€šè¿‡bitsandbytesåŠ è½½4-bitæˆ–8-bité‡åŒ–çš„æ¨¡å‹ï¼Œè¿›ä¸€æ­¥èŠ‚çœGPUæ˜¾å­˜.
  # 4-bit é‡åŒ–çš„ InternLM 7B å¤§çº¦ä¼šæ¶ˆè€— 8GB æ˜¾å­˜.
  # pip install -U bitsandbytes
  # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, load_in_8bit=True)
  # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, load_in_4bit=True)
model = model.eval()
response, history = model.chat(tokenizer, "hello", history=[])
print(response)
response, history = model.chat(tokenizer, "please provide three suggestions about time management", history=history)
print(response)
```

### é€šè¿‡å‰ç«¯ç½‘é¡µå¯¹è¯

å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å¯åŠ¨ä¸€ä¸ªå‰ç«¯çš„ç•Œé¢æ¥ä¸ InternLM Chat 7B æ¨¡å‹è¿›è¡Œäº¤äº’

```bash
pip install streamlit
pip install transformers>=4.34
streamlit run ./chat/web_demo.py
```

### åŸºäº InternLM é«˜æ€§èƒ½éƒ¨ç½²

æˆ‘ä»¬ä½¿ç”¨ [LMDeploy](https://github.com/InternLM/LMDeploy) å®Œæˆ InternLM çš„ä¸€é”®éƒ¨ç½²ã€‚

é€šè¿‡ `pip install lmdeploy>=0.2.1` å®‰è£… LMDeploy ä¹‹åï¼Œåªéœ€ 4 è¡Œä»£ç ï¼Œå°±å¯ä»¥å®ç°ç¦»çº¿æ‰¹å¤„ç†ï¼š

```python
from lmdeploy import pipeline
pipe = pipeline("internlm/internlm2-chat-7b")
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```

è¯·å‚è€ƒ[éƒ¨ç½²æŒ‡å—](./chat/lmdeploy.md)äº†è§£æ›´å¤šä½¿ç”¨æ¡ˆä¾‹ï¼Œæ›´å¤šéƒ¨ç½²æ•™ç¨‹åˆ™å¯åœ¨[è¿™é‡Œ](https://github.com/InternLM/LMDeploy)æ‰¾åˆ°ã€‚

## å¾®è°ƒ&è®­ç»ƒ

è¯·å‚è€ƒ[å¾®è°ƒæ•™ç¨‹](./finetune/)å°è¯•ç»­è®­æˆ–å¾®è°ƒ InternLM2ã€‚

**æ³¨æ„ï¼š** æœ¬é¡¹ç›®ä¸­çš„å…¨é‡è®­ç»ƒåŠŸèƒ½å·²ç»è¿ç§»åˆ°äº† [InternEvo](https://github.com/InternLM/InternEvo) ä»¥ä¾¿ç”¨æˆ·ä½¿ç”¨ã€‚InternEvo æä¾›äº†é«˜æ•ˆçš„é¢„è®­ç»ƒå’Œå¾®è°ƒåŸºå»ºç”¨äºè®­ç»ƒ InternLM ç³»åˆ—æ¨¡å‹ã€‚

## è¯„æµ‹

æˆ‘ä»¬ä½¿ç”¨ [OpenCompass](https://github.com/open-compass/opencompass) è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚åœ¨ InternLM-2 ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦æ ‡å‡†å®¢è§‚è¯„ä¼°ã€é•¿æ–‡è¯„ä¼°ï¼ˆå¤§æµ·æé’ˆï¼‰ã€æ•°æ®æ±¡æŸ“è¯„ä¼°ã€æ™ºèƒ½ä½“è¯„ä¼°å’Œä¸»è§‚è¯„ä¼°ã€‚

### æ ‡å‡†å®¢è§‚è¯„æµ‹

è¯·æŒ‰ç…§ [OpenCompass æ•™ç¨‹](https://opencompass.readthedocs.io/zh-cn/latest/get_started/installation.html) è¿›è¡Œå®¢è§‚è¯„æµ‹ã€‚æˆ‘ä»¬é€šå¸¸åœ¨ Base æ¨¡å‹ä¸Šä½¿ç”¨ ppl è¿›è¡Œå¤šé¡¹é€‰æ‹©é¢˜è¯„æµ‹ï¼Œåœ¨ Chat æ¨¡å‹ä¸Šä½¿ç”¨ gen è¿›è¡Œæ‰€æœ‰é—®é¢˜çš„ç­”æ¡ˆç”Ÿæˆå’Œè¯„æµ‹ã€‚

### é•¿æ–‡è¯„ä¼°ï¼ˆå¤§æµ·æé’ˆï¼‰

æœ‰å…³ `å¤§æµ·æé’ˆ` è¯„ä¼°çš„æ•™ç¨‹ï¼Œè¯·å‚é˜… [æ–‡æ¡£](https://github.com/open-compass/opencompass/blob/main/docs/en/advanced_guides/needleinahaystack_eval.md) ä¸­çš„æ•™ç¨‹ã€‚

### æ•°æ®æ±¡æŸ“è¯„ä¼°

è¦äº†è§£æ›´å¤šå…³äºæ•°æ®æ±¡æŸ“è¯„ä¼°çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ±¡æŸ“è¯„ä¼°](https://opencompass.readthedocs.io/en/latest/advanced_guides/contamination_eval.html)ã€‚

### æ™ºèƒ½ä½“è¯„ä¼°

- è¦è¯„ä¼°å¤§æ¨¡å‹çš„å·¥å…·åˆ©ç”¨èƒ½åŠ›ï¼Œè¯·ä½¿ç”¨ [T-Eval](https://github.com/open-compass/T-Eval) è¿›è¡Œè¯„æµ‹ã€‚
- å¯¹äºä»£ç è§£é‡Šå™¨è¯„ä¼°ï¼Œè¯·ä½¿ç”¨ [gsm-8k-agent](https://github.com/open-compass/opencompass/blob/main/configs/datasets/gsm8k/gsm8k_agent_gen_be1606.py) æä¾›çš„é…ç½®è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜éœ€è¦å®‰è£… [Lagent](https://github.com/InternLM/lagent)ã€‚

### ä¸»è§‚è¯„ä¼°

- è¯·æŒ‰ç…§ [æ•™ç¨‹](https://opencompass.readthedocs.io/en/latest/advanced_guides/subjective_evaluation.html) è¿›è¡Œä¸»è§‚è¯„ä¼°ã€‚

## è´¡çŒ®

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ InternLM æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚éå¸¸æ¬¢è¿ç¤¾åŒºç”¨æˆ·èƒ½å‚ä¸è¿›é¡¹ç›®ä¸­æ¥ã€‚è¯·å‚è€ƒè´¡çŒ®æŒ‡å—æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

InternLM ä»£ç åº“æ˜¯ä¸€æ¬¾ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å’Œæ¥è‡ªä¸åŒé«˜æ ¡ã€ä¼ä¸šçš„ç ”å‘äººå‘˜å…±åŒå‚ä¸è´¡çŒ®çš„å¼€æºé¡¹ç›®ã€‚æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®æä¾›æ–°åŠŸèƒ½æ”¯æŒçš„è´¡çŒ®è€…ï¼Œä»¥åŠæä¾›å®è´µåé¦ˆæ„è§çš„ç”¨æˆ·ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå·¥å…·ç®±å’ŒåŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºç¤¾åŒºæä¾›çµæ´»é«˜æ•ˆçš„ä»£ç å·¥å…·ï¼Œä¾›ç”¨æˆ·å¾®è°ƒ InternLM å¹¶å¼€å‘è‡ªå·±çš„æ–°æ¨¡å‹ï¼Œä»è€Œä¸æ–­ä¸ºå¼€æºç¤¾åŒºæä¾›è´¡çŒ®ã€‚ç‰¹åˆ«é¸£è°¢ [flash-attention](https://github.com/HazyResearch/flash-attention) ä¸ [ColossalAI](https://github.com/hpcaitech/ColossalAI) ä¸¤é¡¹å¼€æºé¡¹ç›®ã€‚

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚

## å¼•ç”¨

```
@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}
```
