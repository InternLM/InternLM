2023-07-25 14:04:04,262	INFO launch.py:77 in args_sanity_check -- gradient_accumulation size will be setted to 4.
2023-07-25 14:04:04,263	INFO launch.py:92 in args_sanity_check -- +++++++++++++++ Data Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:93 in args_sanity_check -- seq_len: 2048
2023-07-25 14:04:04,263	INFO launch.py:94 in args_sanity_check -- micro_num: 4
2023-07-25 14:04:04,263	INFO launch.py:95 in args_sanity_check -- micro_bsz: 2
2023-07-25 14:04:04,263	INFO launch.py:96 in args_sanity_check -- packed_length: 4096
2023-07-25 14:04:04,263	INFO launch.py:97 in args_sanity_check -- pack_sample_into_one: False
2023-07-25 14:04:04,263	INFO launch.py:98 in args_sanity_check -- min_length: 50
2023-07-25 14:04:04,263	INFO launch.py:125 in args_sanity_check -- +++++++++++++++ Ckpt Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:126 in args_sanity_check -- is enable save ckpt: False
2023-07-25 14:04:04,263	INFO launch.py:127 in args_sanity_check -- save_ckpt_folder: None
2023-07-25 14:04:04,263	INFO launch.py:128 in args_sanity_check -- checkpoint_every: 50
2023-07-25 14:04:04,263	INFO launch.py:144 in args_sanity_check -- +++++++++++++++ Other Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:145 in args_sanity_check -- cudnn.benchmark: False
2023-07-25 14:04:04,263	INFO launch.py:146 in args_sanity_check -- cudnn.deterministic: False
2023-07-25 14:04:04,263	INFO launch.py:147 in args_sanity_check -- clip_grad_norm: 1.0
2023-07-25 14:04:04,263	INFO launch.py:161 in args_sanity_check -- +++++++++++++++ Model Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:162 in args_sanity_check -- Model: {'checkpoint': True, 'num_attention_heads': 32, 'embed_split_hidden': True, 'vocab_size': 103168, 'embed_grad_scale': 1, 'parallel_output': True, 'hidden_size': 4096, 'num_layers': 32, 'mlp_ratio': 2.6666666666666665, 'apply_post_layer_norm': False, 'dtype': torch.bfloat16, 'norm_type': 'rmsnorm', 'layer_norm_epsilon': 1e-05}
2023-07-25 14:04:04,263	INFO launch.py:164 in args_sanity_check -- +++++++++++++++ grad_scaler Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:165 in args_sanity_check -- grad_scaler: {'fp16': {'initial_scale': 65536, 'min_scale': 1, 'growth_interval': 1000}, 'growth_factor': 2, 'backoff_factor': 0.5, 'max_scale': 16777216, 'hysteresis': 2}
2023-07-25 14:04:04,263	INFO launch.py:167 in args_sanity_check -- +++++++++++++++ hybrid_zero_optimizer Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:168 in args_sanity_check -- hybrid_zero_optimizer: {'zero_overlap_communication': True, 'reduce_bucket_size': 536870912, 'clip_grad_norm': 1.0}
2023-07-25 14:04:04,263	INFO launch.py:170 in args_sanity_check -- +++++++++++++++ adam Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:171 in args_sanity_check -- adam: {'lr': 0.0001, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_beta2_c': 0, 'adam_eps': 1e-08, 'weight_decay': 0.01}
2023-07-25 14:04:04,263	INFO launch.py:173 in args_sanity_check -- +++++++++++++++ beta2_scheduler Info +++++++++++++++
2023-07-25 14:04:04,263	INFO launch.py:174 in args_sanity_check -- beta2_scheduler: {'init_beta2': 0.95, 'c': 0, 'cur_iter': -1}
2023-07-25 14:04:04,613	INFO parallel_context.py:503 in set_device -- process rank 2 is bound to host:SH-IDC1-10-140-0-138 device: 2
2023-07-25 14:04:04,616	INFO parallel_context.py:503 in set_device -- process rank 1 is bound to host:SH-IDC1-10-140-0-138 device: 1
2023-07-25 14:04:04,620	INFO parallel_context.py:503 in set_device -- process rank 5 is bound to host:SH-IDC1-10-140-0-138 device: 5
2023-07-25 14:04:04,620	INFO parallel_context.py:503 in set_device -- process rank 7 is bound to host:SH-IDC1-10-140-0-138 device: 7
2023-07-25 14:04:04,621	INFO parallel_context.py:503 in set_device -- process rank 3 is bound to host:SH-IDC1-10-140-0-138 device: 3
2023-07-25 14:04:04,622	INFO parallel_context.py:503 in set_device -- process rank 4 is bound to host:SH-IDC1-10-140-0-138 device: 4
2023-07-25 14:04:04,622	INFO parallel_context.py:503 in set_device -- process rank 0 is bound to host:SH-IDC1-10-140-0-138 device: 0
2023-07-25 14:04:04,622	INFO parallel_context.py:503 in set_device -- process rank 6 is bound to host:SH-IDC1-10-140-0-138 device: 6
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 6, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 0, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 1, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 2048,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 2, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 4, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 1024,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 3, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 2048,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,296	INFO launch.py:235 in launch -- Distributed environment is initialized, data parallel size: 4, pipeline parallel size: 2, tensor parallel size: 1
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 5, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 2048,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,296	INFO parallel_context.py:535 in set_seed -- initialized seed on rank 7, numpy: 1024, python random: 1024, ParallelMode.DATA: 1024, ParallelMode.TENSOR: 2048,the default parallel seed is ParallelMode.DATA.
2023-07-25 14:04:10,299	INFO writer.py:61 in init_tb_writer -- Login tensorboard logs to: 7b_train/Jul25_14-04-10
2023-07-25 14:04:10,310	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=0,dp=0===========
2023-07-25 14:04:10,314	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=0,dp=1===========
2023-07-25 14:04:10,315	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=1,dp=0===========
2023-07-25 14:04:10,315	INFO modeling_internlm.py:408 in _build_generic_model_1d -- The layer sharding is [[(0, 16)], [(16, 32)]].
2023-07-25 14:04:10,316	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=0,dp=2===========
2023-07-25 14:04:10,318	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=1,dp=1===========
2023-07-25 14:04:10,319	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=0,dp=3===========
2023-07-25 14:04:10,320	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=1,dp=3===========
2023-07-25 14:04:10,321	INFO train.py:383 in main -- ===========New Run Jul25_14-04-10 on host:SH-IDC1-10-140-0-138,tp:0,pp=1,dp=2===========
Traceback (most recent call last):
  File "train.py", line 517, in <module>
    main(args)
  File "train.py", line 399, in main
    train_dl, _ = get_train_data_loader(num_worker=4)
  File "train.py", line 132, in get_train_data_loader
    train_ds = RandomDataset(num_samples=1000000, max_len=data_cfg.seq_len)
  File "/mnt/petrelfs/xiongyingtong/InternLM/internlm/data/dummy_dataset.py", line 28, in __init__
    d = d[:max_len]
KeyboardInterrupt
slurmstepd: error: *** STEP 7392990.0 ON SH-IDC1-10-140-0-138 CANCELLED AT 2023-07-25T14:05:22 ***
Traceback (most recent call last):
  File "train.py", line 517, in <module>
    main(args)
  File "train.py", line 399, in main
    train_dl, _ = get_train_data_loader(num_worker=4)
  File "train.py", line 132, in get_train_data_loader
    train_ds = RandomDataset(num_samples=1000000, max_len=data_cfg.seq_len)
  File "/mnt/petrelfs/xiongyingtong/InternLM/internlm/data/dummy_dataset.py", line 26, in __init__
    d = list(range(n)) * r
KeyboardInterrupt
Traceback (most recent call last):
  File "train.py", line 517, in <module>
    main(args)
  File "train.py", line 399, in main
    train_dl, _ = get_train_data_loader(num_worker=4)
  File "train.py", line 132, in get_train_data_loader
    train_ds = RandomDataset(num_samples=1000000, max_len=data_cfg.seq_len)
  File "/mnt/petrelfs/xiongyingtong/InternLM/internlm/data/dummy_dataset.py", line 26, in __init__
    d = list(range(n)) * r
KeyboardInterrupt
Traceback (most recent call last):
  File "train.py", line 517, in <module>
    main(args)
  File "train.py", line 399, in main
    train_dl, _ = get_train_data_loader(num_worker=4)
  File "train.py", line 132, in get_train_data_loader
    train_ds = RandomDataset(num_samples=1000000, max_len=data_cfg.seq_len)
  File "/mnt/petrelfs/xiongyingtong/InternLM/internlm/data/dummy_dataset.py", line 26, in __init__
    d = list(range(n)) * r
KeyboardInterrupt
