# InternLM

<div align="center">

<img src="./doc/imgs/logo.svg" width="200"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="5">ä¹¦ç”ŸÂ·æµ¦è¯­ å®˜ç½‘</font></b>
    <sup>
      <a href="https://internlm.intern-ai.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    <div>&nbsp;</div>
  </div>

[![license](./doc/imgs/license.svg)](https://github.com/open-mmlab/mmdetection/blob/main/LICENSE)
[![evaluation](./doc/imgs/compass_support.svg)](https://github.com/internLM/OpenCompass/)
[![Documentation Status](https://readthedocs.org/projects/internlm/badge/?version=latest)](https://internlm.readthedocs.io/zh_CN/latest/?badge=latest)

[ğŸ“˜ä½¿ç”¨æ–‡æ¡£](./doc/usage.md) |
[ğŸ› ï¸å®‰è£…æ•™ç¨‹](./doc/install.md) |
[ğŸ“Šè®­ç»ƒæ€§èƒ½](./doc/train_performance.md) |
[ğŸ‘€æ¨¡å‹åº“](#model-zoo) |
[ğŸ¤—HuggingFace](https://huggingface.co/spaces/internlm/InternLM-Chat-7B) |
[ğŸ†•Update News](./CHANGE_LOG.md) |
[ğŸ¤”Reporting Issues](https://github.com/InternLM/InternLM/issues/new)

[English](./README.md) |
[ç®€ä½“ä¸­æ–‡](./README-zh-Hans.md) |
[æ—¥æœ¬èª](./README-ja-JP.md)

</div>

<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„<a href="https://twitter.com/intern_lm" target="_blank">æ¨ç‰¹</a>ã€<a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> å’Œ <a href="https://r.vansin.top/?r=internwx" target="_blank">å¾®ä¿¡ç¤¾åŒº</a>
</p>

## ç®€ä»‹

InternLM ï¼Œå³ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹ï¼ŒåŒ…å«é¢å‘å®ç”¨åœºæ™¯çš„70äº¿å‚æ•°åŸºç¡€æ¨¡å‹ä¸å¯¹è¯æ¨¡å‹ ï¼ˆInternLM-7Bï¼‰ã€‚æ¨¡å‹å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- ä½¿ç”¨ä¸Šä¸‡äº¿é«˜è´¨é‡è¯­æ–™ï¼Œå»ºç«‹æ¨¡å‹è¶…å¼ºçŸ¥è¯†ä½“ç³»ï¼›
- æ”¯æŒ8kè¯­å¢ƒçª—å£é•¿åº¦ï¼Œå®ç°æ›´é•¿è¾“å…¥ä¸æ›´å¼ºæ¨ç†ä½“éªŒï¼›
- é€šç”¨å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œæ”¯æŒç”¨æˆ·çµæ´»è‡ªåŠ©æ­å»ºæµç¨‹ï¼›

æä¾›äº†æ”¯æŒæ¨¡å‹é¢„è®­ç»ƒçš„è½»é‡çº§è®­ç»ƒæ¡†æ¶ï¼Œæ— éœ€å®‰è£…å¤§é‡ä¾èµ–åŒ…ï¼Œä¸€å¥—ä»£ç æ”¯æŒåƒå¡é¢„è®­ç»ƒå’Œå•å¡äººç±»åå¥½å¯¹é½è®­ç»ƒï¼ŒåŒæ—¶å®ç°äº†æè‡´çš„æ€§èƒ½ä¼˜åŒ–ï¼Œå®ç°åƒå¡è®­ç»ƒä¸‹è¿‘90%åŠ é€Ÿæ•ˆç‡ã€‚

## æ–°é—»

æˆ‘ä»¬å¼€æºäº† InternLM-Chat-7B v1.1ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè°ƒç”¨ä»£ç è§£é‡Šå™¨å’Œå·¥å…·æ’ä»¶ã€‚ä½ å¯ä»¥åœ¨ [Lagent](https://github.com/InternLM/lagent) ä¸­ä½“éªŒè¿™äº›æ–°åŠŸèƒ½ã€‚

## InternLM-7B

### æ€§èƒ½è¯„æµ‹

æˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…· [OpenCompass](https://github.com/internLM/OpenCompass/) ä»å­¦ç§‘ç»¼åˆèƒ½åŠ›ã€è¯­è¨€èƒ½åŠ›ã€çŸ¥è¯†èƒ½åŠ›ã€æ¨ç†èƒ½åŠ›ã€ç†è§£èƒ½åŠ›äº”å¤§èƒ½åŠ›ç»´åº¦å¯¹InternLMå¼€å±•å…¨é¢è¯„æµ‹ï¼Œéƒ¨åˆ†è¯„æµ‹ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼Œæ¬¢è¿è®¿é—®[OpenCompass æ¦œå•](https://opencompass.org.cn/rank)è·å–æ›´å¤šçš„è¯„æµ‹ç»“æœã€‚

| æ•°æ®é›†\æ¨¡å‹           |  **InternLM-Chat-7B** |  **InternLM-7B**  |  LLaMA-7B | Baichuan-7B | ChatGLM2-6B | Alpaca-7B | Vicuna-7B |
| -------------------- | --------------------- | ---------------- | --------- |  --------- | ------------ | --------- | ---------- |
| C-Eval(Val)          |      53.2             |        53.4       | 24.2      | 42.7       |  50.9       |  28.9     | 31.2     |
| MMLU                 |      50.8             |       51.0        | 35.2*     |  41.5      |  46.0       |  39.7     | 47.3     |
| AGIEval              |      42.5             |       37.6        | 20.8      | 24.6       |  39.0       | 24.1      | 26.4     |
| CommonSenseQA        |      75.2             |      59.5         | 65.0      | 58.8       | 60.0        | 68.7      | 66.7     |
| BUSTM                |      74.3             |       50.6        | 48.5      | 51.3        | 55.0        | 48.8      | 62.5     |
| CLUEWSC              |      78.6             |      59.1         |  50.3     |  52.8     |  59.8     |   50.3    |  52.2     |
| MATH                 |      6.4            |         7.1        |  2.8       | 3.0       | 6.6       |  2.2      | 2.8       |
| GSM8K                |      34.5           |        31.2        | 10.1       | 9.7       | 29.2      |  6.0      | 15.3  |
|  HumanEval           |      14.0           |        10.4        |   14.0     | 9.2       | 9.2       | 9.2       | 11.0  |
| RACE(High)           |      76.3           |        57.4        | 46.9*      | 28.1      | 66.3      | 40.7      | 54.0  |

- ä»¥ä¸Šè¯„æµ‹ç»“æœåŸºäº [OpenCompass 20230706](https://github.com/internLM/OpenCompass/) è·å¾—ï¼ˆéƒ¨åˆ†æ•°æ®æ ‡æ³¨`*`ä»£è¡¨æ•°æ®æ¥è‡ªåŸå§‹è®ºæ–‡ï¼‰ï¼Œå…·ä½“æµ‹è¯•ç»†èŠ‚å¯å‚è§ [OpenCompass](https://github.com/internLM/OpenCompass/) ä¸­æä¾›çš„é…ç½®æ–‡ä»¶ã€‚
- è¯„æµ‹æ•°æ®ä¼šå›  [OpenCompass](https://github.com/internLM/OpenCompass/) çš„ç‰ˆæœ¬è¿­ä»£è€Œå­˜åœ¨æ•°å€¼å·®å¼‚ï¼Œè¯·ä»¥ [OpenCompass](https://github.com/internLM/OpenCompass/) æœ€æ–°ç‰ˆçš„è¯„æµ‹ç»“æœä¸ºä¸»ã€‚

### Model Zoo

å½“å‰é€šè¿‡ InternLM è®­ç»ƒçš„ InternLM 7B å’Œ InternLM 7B Chat å·²ç»å¼€æºï¼Œæˆ‘ä»¬æä¾›ä¸¤ç§æ ¼å¼çš„æ¨¡å‹æƒé‡ä»¥ä¾›ä½¿ç”¨ã€‚é™¤äº†ä½¿ç”¨ Transformers æ ¼å¼åŠ è½½æ¨¡å‹ä¹‹å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡ InternLM åŠ è½½ä»¥ä¸‹æ ¼å¼çš„æƒé‡ç›´æ¥è¿›è¡Œç»§ç»­é¢„è®­ç»ƒæˆ–äººç±»åå¥½å¯¹é½è®­ç»ƒ

| æ¨¡å‹                 | InternLM æ ¼å¼æƒé‡ä¸‹è½½åœ°å€                                                                                                                      | Transformers æ ¼å¼æƒé‡ä¸‹è½½åœ°å€                    |
| -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ |
| **InternLM 7B**      | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-7b) | [ğŸ¤—internlm/intern-7b](https://huggingface.co/internlm/internlm-7b) |
| **InternLM Chat 7B v1.1**    | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-chat-7b-v1.1)    | [ğŸ¤—internlm/intern-chat-7b-v1.1](https://huggingface.co/internlm/internlm-chat-7b-v1.1)       |
| **InternLM Chat 7B** | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-chat-7b) | [ğŸ¤—internlm/intern-chat-7b](https://huggingface.co/internlm/internlm-chat-7b)
| **InternLM Chat 7B 8k** | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-chat-7b-8k) | [ğŸ¤—internlm/intern-chat-7b-8k](https://huggingface.co/internlm/internlm-chat-7b-8k)

**å±€é™æ€§ï¼š** å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºæ¨¡å‹å¤§å°ä»¥åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›å¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚

### é€šè¿‡ Transformers åŠ è½½

é€šè¿‡ä»¥ä¸‹çš„ä»£ç åŠ è½½ InternLM 7B Chat æ¨¡å‹

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> tokenizer = AutoTokenizer.from_pretrained("internlm/internlm-chat-7b-v1_1", trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("internlm/internlm-chat-7b-v1_1", trust_remote_code=True).cuda()
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
>>> response, history = model.chat(tokenizer, "è¯·æä¾›ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ã€‚", history=history)
>>> print(response)
å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ï¼š
1. åˆ¶å®šè®¡åˆ’ï¼šåˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„è®¡åˆ’ï¼ŒåŒ…æ‹¬æ¯å¤©è¦å®Œæˆçš„ä»»åŠ¡å’Œæ´»åŠ¨ã€‚è¿™å°†æœ‰åŠ©äºæ‚¨æ›´å¥½åœ°ç»„ç»‡æ—¶é—´ï¼Œå¹¶ç¡®ä¿æ‚¨èƒ½å¤ŸæŒ‰æ—¶å®Œæˆä»»åŠ¡ã€‚
2. ä¼˜å…ˆçº§ï¼šå°†ä»»åŠ¡æŒ‰ç…§ä¼˜å…ˆçº§æ’åºï¼Œå…ˆå®Œæˆæœ€é‡è¦çš„ä»»åŠ¡ã€‚è¿™å°†ç¡®ä¿æ‚¨èƒ½å¤Ÿåœ¨æœ€çŸ­çš„æ—¶é—´å†…å®Œæˆæœ€é‡è¦çš„ä»»åŠ¡ï¼Œä»è€ŒèŠ‚çœæ—¶é—´ã€‚
3. é›†ä¸­æ³¨æ„åŠ›ï¼šé¿å…åˆ†å¿ƒï¼Œé›†ä¸­æ³¨æ„åŠ›å®Œæˆä»»åŠ¡ã€‚å…³é—­ç¤¾äº¤åª’ä½“å’Œç”µå­é‚®ä»¶é€šçŸ¥ï¼Œä¸“æ³¨äºä»»åŠ¡ï¼Œè¿™å°†å¸®åŠ©æ‚¨æ›´å¿«åœ°å®Œæˆä»»åŠ¡ï¼Œå¹¶å‡å°‘é”™è¯¯çš„å¯èƒ½æ€§ã€‚
```

### é€šè¿‡å‰ç«¯ç½‘é¡µå¯¹è¯

å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å¯åŠ¨ä¸€ä¸ªå‰ç«¯çš„ç•Œé¢æ¥ä¸ InternLM Chat 7B æ¨¡å‹è¿›è¡Œäº¤äº’

```bash
pip install streamlit==1.24.0
pip install transformers==4.30.2
streamlit run web_demo.py
```

æ•ˆæœå¦‚ä¸‹

![æ•ˆæœ](https://github.com/InternLM/InternLM/assets/9102141/11b60ee0-47e4-42c0-8278-3051b2f17fe4)

### åŸºäºInternLMé«˜æ€§èƒ½éƒ¨ç½²

æˆ‘ä»¬ä½¿ç”¨ [LMDeploy](https://github.com/InternLM/LMDeploy) å®Œæˆ InternLM çš„ä¸€é”®éƒ¨ç½²ã€‚

```bash
python3 -m pip install lmdeploy
```

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯ä»¥åœ¨ç»ˆç«¯ä¸ `internlm-chat-7b` æ¨¡å‹è¿›è¡Œäº¤äº’å¼å¯¹è¯ï¼Œæˆ–è€…é€šè¿‡ WebUI ä¸å®ƒèŠå¤©ã€‚

```bash
# è½¬æ¢æƒé‡æ ¼å¼
python3 -m lmdeploy.serve.turbomind.deploy internlm-chat-7b

# åœ¨ç»ˆç«¯è¿›è¡Œäº¤äº’å¼å¯¹è¯
python3 -m lmdeploy.turbomind.chat ./workspace

# å¯åŠ¨ gradio æœåŠ¡
python3 -m lmdeploy.serve.gradio.app ./workspace
```
ä»¥ä¸Šè¿‡ç¨‹ä¸­ï¼ŒLMDeploy ä½¿ç”¨çš„æ˜¯ FP16 çš„è®¡ç®—ç²¾åº¦ã€‚

é™¤äº† FP16 ç²¾åº¦ï¼ŒLMDeploy è¿˜æ”¯æŒ `internlm-chat-7b` 4bit æƒé‡æ¨¡å‹æ¨ç†ã€‚å®ƒä¸ä»…æŠŠæ¨¡å‹çš„æ˜¾å­˜å‡å°‘åˆ° 6Gï¼Œå¤§çº¦åªæœ‰ FP16 çš„ 40%ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œç»è¿‡ kernel å±‚é¢çš„æè‡´ä¼˜åŒ–ï¼Œå…¶æ¨ç†æ€§èƒ½åœ¨ A100-80G ä¸Šå¯è¾¾åˆ° FP16 çš„ 2.4 å€ä»¥ä¸Šã€‚

ä»¥ä¸‹æ˜¯`internlm-chat-7b` 4bit æƒé‡æ¨¡å‹çš„éƒ¨ç½²æ–¹æ³•ã€‚æ¨ç†é€Ÿåº¦çš„ bechmark è¯·å‚è€ƒ[è¿™é‡Œ](https://github.com/InternLM/lmdeploy/blob/main/docs/zh_cn/w4a16.md#%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6)

```bash
# download prequnantized internlm-chat-7b model from huggingface
git-lfs install
git clone https://huggingface.co/lmdeploy/llama2-chat-7b-w4

# Convert the model's layout and store it in the default path, ./workspace.
python3 -m lmdeploy.serve.turbomind.deploy internlm-chat-7b ./llama2-chat-7b-w4 awq --group-size 128

# inference lmdeploy's turbomind engine
python3 -m lmdeploy.turbomind.chat ./workspace

# serving with gradio
python3 -m lmdeploy.serve.gradio.app ./workspace
```
LMDeploy æ˜¯æ¶µç›–äº† LLM ä»»åŠ¡çš„å…¨å¥—è½»é‡åŒ–ã€éƒ¨ç½²å’ŒæœåŠ¡çš„å·¥å…·ç®±ã€‚è¯·å‚è€ƒ [éƒ¨ç½²æ•™ç¨‹](https://github.com/InternLM/LMDeploy) äº†è§£ InternLM çš„æ›´å¤šéƒ¨ç½²ç»†èŠ‚ã€‚


## å¾®è°ƒ&è®­ç»ƒ

### é¢„è®­ç»ƒä¸å¾®è°ƒä½¿ç”¨æ•™ç¨‹

è¯·å‚è€ƒ[ä½¿ç”¨æ•™ç¨‹](./doc/usage.md)å¼€å§‹InternLMçš„å®‰è£…ã€æ•°æ®å¤„ç†ã€é¢„è®­ç»ƒä¸å¾®è°ƒã€‚

### è½¬æ¢ä¸º Transformers æ ¼å¼ä½¿ç”¨

é€šè¿‡ InternLM è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¾ˆè½»æ¾åœ°è½¬æ¢ä¸º HuggingFace Transformers æ ¼å¼ï¼Œæ–¹ä¾¿ä¸ç¤¾åŒºå„ç§å¼€æºé¡¹ç›®æ— ç¼å¯¹æ¥ã€‚å€ŸåŠ© `tools/transformers/convert2hf.py` å¯ä»¥å°†è®­ç»ƒä¿å­˜çš„æƒé‡ä¸€é”®è½¬æ¢ä¸º transformers æ ¼å¼

```bash
python tools/transformers/convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer ./tools/V7_sft.model
```

è½¬æ¢ä¹‹åå¯ä»¥é€šè¿‡ä»¥ä¸‹çš„ä»£ç åŠ è½½ä¸º transformers

```python
>>> from transformers import AutoTokenizer, AutoModel
>>> model = AutoModel.from_pretrained("hf_ckpt/", trust_remote_code=True).cuda()
```

## è®­ç»ƒç³»ç»Ÿ

### ç³»ç»Ÿç»“æ„

è¯·å‚è€ƒ[ç³»ç»Ÿç»“æ„æ–‡æ¡£](./doc/structure.md)è¿›ä¸€æ­¥äº†è§£ã€‚

### è®­ç»ƒæ€§èƒ½

InternLM æ·±åº¦æ•´åˆäº† Flash-Attention, Apex ç­‰é«˜æ€§èƒ½æ¨¡å‹ç®—å­ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡æ„å»º Hybrid Zero æŠ€æœ¯ï¼Œå®ç°è®¡ç®—å’Œé€šä¿¡çš„é«˜æ•ˆé‡å ï¼Œå¤§å¹…é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„è·¨èŠ‚ç‚¹é€šä¿¡æµé‡ã€‚InternLM æ”¯æŒ 7B æ¨¡å‹ä» 8 å¡æ‰©å±•åˆ° 1024 å¡ï¼Œåƒå¡è§„æ¨¡ä¸‹åŠ é€Ÿæ•ˆç‡å¯é«˜è¾¾ 90%ï¼Œè®­ç»ƒååè¶…è¿‡ 180TFLOPSï¼Œå¹³å‡å•å¡æ¯ç§’å¤„ç†çš„ token æ•°é‡è¶…è¿‡3600ã€‚ä¸‹è¡¨ä¸º InternLM åœ¨ä¸åŒé…ç½®ä¸‹çš„æ‰©å±•æ€§æµ‹è¯•æ•°æ®ï¼š

| GPU Number         | 8   | 16  | 32  | 64  | 128  | 256  | 512  | 1024  |
| ---------------- | ---- | ---- | ---- | ---- | ----- | ----- | ----- | ------ |
| TGS | 4078 | 3939 | 3919 | 3944 | 3928  | 3920  | 3835  | 3625   |
| TFLOPS  | 193 | 191  | 188  | 188  | 187   | 185   | 186   | 184    |

TGS ä»£è¡¨å¹³å‡æ¯GPUæ¯ç§’å¯ä»¥å¤„ç†çš„ Token æ•°é‡ã€‚æ›´å¤šçš„æ€§èƒ½æµ‹è¯•æ•°æ®å¯å‚è€ƒ[è®­ç»ƒæ€§èƒ½æ–‡æ¡£](./doc/train_performance.md)è¿›ä¸€æ­¥äº†è§£ã€‚

## è´¡çŒ®

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ InternLM æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚éå¸¸æ¬¢è¿ç¤¾åŒºç”¨æˆ·èƒ½å‚ä¸è¿›é¡¹ç›®ä¸­æ¥ã€‚è¯·å‚è€ƒè´¡çŒ®æŒ‡å—æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

## è‡´è°¢

InternLM ä»£ç åº“æ˜¯ä¸€æ¬¾ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å’Œæ¥è‡ªä¸åŒé«˜æ ¡ã€ä¼ä¸šçš„ç ”å‘äººå‘˜å…±åŒå‚ä¸è´¡çŒ®çš„å¼€æºé¡¹ç›®ã€‚æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®æä¾›æ–°åŠŸèƒ½æ”¯æŒçš„è´¡çŒ®è€…ï¼Œä»¥åŠæä¾›å®è´µåé¦ˆçš„ç”¨æˆ·ã€‚ æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå·¥å…·ç®±å’ŒåŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºç¤¾åŒºæä¾›çµæ´»é«˜æ•ˆçš„ä»£ç å·¥å…·ï¼Œä¾›ç”¨æˆ·å¾®è°ƒ InternLM å¹¶å¼€å‘è‡ªå·±çš„æ–°æ¨¡å‹ï¼Œä»è€Œä¸æ–­ä¸ºå¼€æºç¤¾åŒºæä¾›è´¡çŒ®ã€‚ç‰¹åˆ«é¸£è°¢[flash-attention](https://github.com/HazyResearch/flash-attention) ä¸ [ColossalAI](https://github.com/hpcaitech/ColossalAI) ä¸¤é¡¹å¼€æºé¡¹ç›®ã€‚

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚

## å¼•ç”¨

```
@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}
```
