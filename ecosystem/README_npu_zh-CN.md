# InternLM-NPU

<div align="center">

<img src="../assets/logo.svg" width="200"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="5">ä¹¦ç”ŸÂ·æµ¦è¯­ å®˜ç½‘</font></b>
    <sup>
      <a href="https://internlm.intern-ai.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    <div>&nbsp;</div>
  </div>

[![license](../assets/license.svg)](https://github.com/open-mmlab/mmdetection/blob/main/LICENSE)
[![evaluation](../assets/compass_support.svg)](https://github.com/internLM/OpenCompass/)

<!-- [![Documentation Status](https://readthedocs.org/projects/internlm/badge/?version=latest)](https://internlm.readthedocs.io/zh_CN/latest/?badge=latest) -->

[ğŸ“˜å•†ä¸šæˆæƒ](#å¼€æºè®¸å¯è¯) |
[ğŸ¤—HuggingFace](https://huggingface.co/internlm) |
[ğŸ†•æœ€æ–°æ¶ˆæ¯](#æ›´æ–°) |
[ğŸ¤”æäº¤åé¦ˆ](https://github.com/InternLM/InternLM/issues/new)|
[ğŸ“œæŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2403.17297)<br>
[ğŸ’¬èŠå¤©åº”ç”¨](https://internlm-chat.intern-ai.org.cn/) |
[ğŸ”—API](https://internlm.intern-ai.org.cn/api/document) |
[ğŸ§©é­”ä¹ç¤¾åŒº](https://modelers.cn/spaces/MindSpore-Lab/INTERNLM2-20B-PLAN)

[English](README_npu.md) |
[ç®€ä½“ä¸­æ–‡](README_npu_zh-CN.md)

</div>

## ä»‹ç»
è¿™æ˜¯ä¸€ä»½ä½¿ç”¨ Ascend NPU å¯¹ InternLM ç³»åˆ—æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæ¨ç†çš„æŒ‡å—ã€‚

## News
\[2025.01.15\] InternLM3-8B-Instruct å¯ç”¨äº Xtunerã€LLaMA-Factoryã€transformers å’Œ openMind ä¸­ã€‚

## Model Zoo

### InternLM3

| Model                     | Transformers                                         | ModelScope                                                                                                                                                              | Modelers                                          | Release Date |
| ------------------------- | ---------------------------------------------------- |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------| ------------------------------------------------- | ------------ |
| **InternLM3-8B-Instruct** | [ğŸ¤—internlm3_8B_instruct](https://huggingface.co/internlm/internlm3-8b-instruct) | [<img src="../assets/modelscope_logo.png" width="20px" /> internlm3_8b_instruct](https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm3-8b-instruct/summary) | [![Open in Modelers](https://modelers.cn/assets/logo1-1bf58310.svg)](https://modelers.cn/models/Intern/internlm3-8b-instruct) | 2025-01-15   |

## ç¯å¢ƒå‡†å¤‡

### å®‰è£…Ascend CANN Toolkitå’ŒKernels

å®‰è£…æ–¹æ³•è¯·å‚è€ƒ[å®‰è£…æ•™ç¨‹](https://gitee.com/link?target=https%3A%2F%2Fwww.hiascend.com%2Fdocument%2Fdetail%2Fzh%2FCANNCommunityEdition%2F80RC2alpha002%2Fquickstart%2Fquickstart%2Fquickstart_18_0004.html)æˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤

```shell
# è¯·æ›¿æ¢URLä¸ºCANNç‰ˆæœ¬å’Œè®¾å¤‡å‹å·å¯¹åº”çš„URL
# å®‰è£…CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-"$(uname -i)".run
bash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-"$(uname -i)".run --install

# å®‰è£…CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
bash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install

# è®¾ç½®ç¯å¢ƒå˜é‡
source /usr/local/Ascend/ascend-toolkit/set_env.sh
```

## Xtuner

### å®‰è£… Xtuner

```shell
git clone https://github.com/InternLM/xtuner.git
cd xtuner
```

ä¿®æ”¹`requirements/runtime.txt`ï¼Œä¿®æ”¹ç‚¹å¦‚ä¸‹ï¼š

```text
bitsandbytes==0.42.0
torchvision==0.19.0
numpy==1.26.4
```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼š

```shell
pip install -e '.[all]'
```

**æ³¨æ„**:

- é»˜è®¤å®‰è£…`torch`ä¸ºæœ€æ–°ç‰ˆï¼Œè¯·æ³¨æ„ä¸`torch_npu`ç‰ˆæœ¬ç›¸åŒ¹é…

### LoRA å¾®è°ƒ

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¤åˆ¶å¹¶é‡å‘½åæ–‡ä»¶ä¸º`internlm3_8b_instruct_lora_oasst1_e10.py`ï¼Œ 

```shell
xtuner copy-cfg internlm2_5_chat_7b_qlora_oasst1_e3 .
mv internlm2_5_chat_7b_qlora_oasst1_e3_copy.py internlm3_8b_instruct_lora_oasst1_e10.py
```

`internlm3_8b_instruct_lora_oasst1_e10.py`é…ç½®æ–‡ä»¶çš„ä¿®æ”¹ç‚¹å¦‚ä¸‹ï¼š

```python
pretrained_model_name_or_path = 'internlm/internlm3-8b-instruct'

max_epochs = 10

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16),
        # quantization_config=dict(
        #     type=BitsAndBytesConfig,
        #     load_in_4bit=True,
        #     load_in_8bit=False,
        #     llm_int8_threshold=6.0,
        #     llm_int8_has_fp16_weight=False,
        #     bnb_4bit_compute_dtype=torch.float16,
        #     bnb_4bit_use_double_quant=True,
        #     bnb_4bit_quant_type='nf4')),

randomness = dict(seed=123, deterministic=True)
```

é€šè¿‡ä¸‹åˆ—å‘½ä»¤å¯åŠ¨å•æœº8å¡å¾®è°ƒï¼š

```shell
NPROC_PER_NODE=8 xtuner train internlm3_8b_instruct_lora_oasst1_e10.py --deepspeed deepspeed_zero2
```

å¾®è°ƒåç»“æœä¿å­˜åœ¨`./work_dirs/internlm3_8b_instruct_lora_oasst1_e10/iter_xxx.pth`ï¼ŒNPUä¸GPUçš„losså¯¹æ¯”å¦‚ä¸‹ï¼š

![xtuner_training_loss](../assets/npu/xtuner_training_loss_compare.png)

### æ¨¡å‹è½¬æ¢

å°†è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸º Hugging Face æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶ï¼Œä¾¿äºåç»­çš„éƒ¨ç½²å’Œä½¿ç”¨ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œè½¬æ¢ï¼š

```shell
xtuner convert pth_to_hf internlm3_8b_instruct_lora_oasst1_e10.py ./work_dirs/internlm3_8b_instruct_lora_oasst1_e10/iter_xxx.pth ./work_dirs/convert_output
```

### æ¨¡å‹åˆå¹¶

LoRAæˆ–QLoRAå¾®è°ƒç”Ÿæˆçš„æ˜¯ä¸€ä¸ªé¢å¤–çš„ `Adapter` å±‚ï¼Œéœ€è¦ä¸åŸæ¨¡å‹åˆå¹¶æ‰èƒ½ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨¡å‹åˆå¹¶ï¼Œå…¶ä¸­`$model_path`
ä¸ºåŸæ¨¡å‹å­˜å‚¨çš„æœ¬åœ°è·¯å¾„, `--max-shard-size 2GB` é™åˆ¶æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§ä¸º2GBï¼š

```shell
xtuner convert merge $model_path ./work_dirs/convert_output ./work_dirs/merge_output --max-shard-size 2GB
```

### å¯¹è¯

ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹æƒé‡è¿›è¡Œå¯¹è¯ï¼š

```shell
cp path_to_your_model/modeling_internlm3.py ./work_dirs/merge_output
xtuner chat ./work_dirs/merge_output --prompt-template internlm2_chat
```

## LLaMA-Factory

### å®‰è£… LLaMA-Factory

```shell
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch-npu,metrics]"
```

### æ¨ç†

åœ¨ LLaMA-Factory è·¯å¾„ä¸‹æ–°å»º`examples/inference/internlm3_8b_instruct.yaml`æ¨ç†é…ç½®æ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹ä¸ºï¼š

```yaml
model_name_or_path: xxx # Support only local loading. Set this parameter to the local weight path of InternLM3-8B-Instruct.
trust_remote_code: true
template: intern3
```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ï¼š

```shell
llamafactory-cli chat examples/inference/internlm3_8b_instruct.yaml
```

### å¾®è°ƒ

åœ¨ LLaMA-Factory è·¯å¾„ä¸‹æ–°å»º`examples/train_full/internlm3_8b_instruct_full_sft.yaml`å¾®è°ƒé…ç½®æ–‡ä»¶ï¼Œå¾®è°ƒé…ç½®æ–‡ä»¶å¦‚ä¸‹ï¼š

```yaml
### model
model_name_or_path: xxx # Support only local loading. Set this parameter to the local weight path of InternLM3-8B-Instruct.
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full
deepspeed: examples/deepspeed/ds_z3_config.json  # choices: [ds_z0_config.json, ds_z2_config.json, ds_z3_config.json]

### dataset
dataset: alpaca_data
template: intern3
cutoff_len: 4096
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: saves/interlm3/full/sft
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 1.0e-6
num_train_epochs: 1.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 5000000000
```

é€šè¿‡ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨å¾®è°ƒï¼š

```shell
llamafactory-cli train examples/train_full/internlm3_8b_instruct_full_sft.yaml
```

### ç²¾åº¦

å¾®è°ƒåå¾—åˆ°çš„lossæ›²çº¿å¦‚ä¸‹ï¼š

![training_loss](../assets/npu/lf_training_loss_npu.png)

ä¸GPUå¯¹æ¯”çš„lossæ›²çº¿å¦‚ä¸‹ï¼š

![training_loss_compare](../assets/npu/lf_training_loss_compare.png)

## Transformers

### æ¨ç†

æ–°å»ºæ¨ç†è„šæœ¬`inference_internlm3_instruct_8b.py`ï¼Œæ¨ç†è„šæœ¬å†…å®¹ä¸ºï¼š

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_dir = "internlm/internlm3-8b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
# `torch_dtype=torch.float16`å¯ä»¥ä»¤æ¨¡å‹ä»¥float16ç²¾åº¦åŠ è½½ï¼Œå¦åˆ™transformersä¼šå°†æ¨¡å‹åŠ è½½ä¸ºfloat32ï¼Œå¯¼è‡´æ˜¾å­˜ä¸è¶³
model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, torch_dtype=torch.float16).npu()
# ï¼ˆå¯é€‰ï¼‰å¦‚æœåœ¨ä½èµ„æºè®¾å¤‡ä¸Šï¼Œå¯ä»¥é€šè¿‡bitsandbytesä»¥4ä½æˆ–8ä½åŠ è½½æ¨¡å‹ï¼Œä»è€Œè¿›ä¸€æ­¥èŠ‚çœGPUå†…å­˜ã€‚
  # InternLM3 8Bä»¥4ä½åŠ è½½å°†å‡ ä¹å ç”¨8GBçš„GPUå†…å­˜.
  # pip install -U bitsandbytes
  # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, load_in_8bit=True).npu()
  # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, load_in_4bit=True).npu()
model = model.eval()
system_prompt = """You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡."""
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "Please tell me five scenic spots in Shanghai"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").npu
generated_ids = model.generate(tokenized_chat, max_new_tokens=1024, temperature=1, repetition_penalty=1.005, top_k=40, top_p=0.8)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_chat, generated_ids)
]
prompt = tokenizer.batch_decode(tokenized_chat)[0]
print(prompt)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)
```

æ‰§è¡Œæ¨ç†è„šæœ¬ï¼š

```shell
python inference_internlm3_instruct_8b.py
```

## openMind Library

### openMind ç®€ä»‹

openMind Library æ˜¯ä¸€ä¸ªå¼€æºçš„å¤§æ¨¡å‹å¥—ä»¶ï¼ŒåŸç”Ÿæ”¯æŒåœ¨æ˜‡è…¾NPUä¸Šè¿›è¡Œå¾®è°ƒã€æ¨ç†ã€è¯„ä¼°å’Œéƒ¨ç½²ã€‚
openMind Library æä¾›é«˜æ˜“ç”¨æ€§çš„æ¥å£å’Œä½¿ç”¨æ–¹å¼ï¼Œå……åˆ†å‘æŒ¥æ˜‡è…¾NPUçš„æ€§èƒ½ï¼Œå¿«é€Ÿæ”¯æŒã€å¢å¼ºä¸šç•Œå‰æ²¿æ¨¡å‹ã€‚

### å¾®è°ƒ

openMind Library æä¾›äº†æ˜‡è…¾ NPU ä¸Šçš„ä¸€é”®å¼æ¨¡å‹å¾®è°ƒæ–¹æ¡ˆï¼Œæ¶µç›–äº†æ•°æ®å¤„ç†ã€å¤šç«™ç‚¹æƒé‡åŠ è½½ï¼Œä½å‚å¾®è°ƒï¼ˆLoRAï¼‰ã€
é‡åŒ–é€‚é…ï¼ˆQLoRAï¼‰ç­‰èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒopenMind Libraryæ”¯æŒæ˜‡è…¾NPUèåˆç®—å­ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹è®­ç»ƒæ€§èƒ½ã€‚

#### å®‰è£… openMind Library

```shell
git clone -b dev https://gitee.com/ascend/openmind.git
cd openmind
pip install -e .[pt]
```

#### å¯åŠ¨å¾®è°ƒ

åœ¨ openmind æ–‡ä»¶å¤¹ä¸‹ï¼Œé€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œå³å¯å¯åŠ¨å¾®è°ƒï¼š

```
openmind-cli train examples/internlm3/train_sft_full_internlm3.yaml
```

#### è®­ç»ƒç»“æœä¸ä¼˜åŠ¿

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒopenMind Library çš„è®­ç»ƒ loss æ­£å¸¸æ”¶æ•›ï¼ŒåŒæ—¶å’Œ GPU å¯¹æ¯”ï¼Œå¹³å‡ç›¸å¯¹è¯¯å·®åœ¨ 2% ä»¥å†…ã€‚

<div align=center>
  <img src="../assets/npu/openmind_train_loss_compare.png" width="600px">
</div>

<p align="center"><strong>ç²¾åº¦å¯¹æ¯”</strong> (npu=8, per_device_train_batch_size=6, max_length=1024)</p>

openMind Library æ”¯æŒåœ¨æ˜‡è…¾ NPU ä¸Šä½¿èƒ½ LoRAã€QLoRA ç­‰å¾®è°ƒæ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘ device å†…å­˜ä½¿ç”¨ã€‚
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé€šè¿‡ä½¿èƒ½ QloRA å¾®è°ƒæ–¹å¼å¯å‡å°‘ device å†…å­˜çº¦ 40%ã€‚

<div align=center>
  <img src="../assets/npu/openmind_train_memory.png" width="400px">
</div>

<p align="center"><strong>Full/LoRA/QLoRA æ˜¾å­˜å¼€é”€</strong> (npu=8, per_device_train_batch_size=6, max_length=1024)</p>

openMind Library æ”¯æŒè®­ç»ƒæ—¶è‡ªåŠ¨åŠ è½½æ˜‡è…¾ NPU èåˆç®—å­ï¼Œæ— éœ€å¼€å‘è€…æ‰‹åŠ¨ä¿®æ”¹ä»£ç æˆ–é…ç½®ï¼Œæå‡æ¨¡å‹è®­ç»ƒæ€§èƒ½
çš„åŒæ—¶å…¼é¡¾æ˜“ç”¨æ€§ã€‚ä¸‹å›¾å±•ç¤ºäº† openMind é»˜è®¤ä½¿èƒ½æ˜‡è…¾ NPU èåˆç®—å­ä¹‹åçš„æ€§èƒ½æ”¶ç›Šã€‚

<div align=center>
  <img src="../assets/npu/openmind_fused_ops.png" width="300px">
</div>

<p align="center"><strong>æ¯ç§’è®­ç»ƒæ ·æœ¬æ•°</strong></p>

æ›´å¤šç‰¹æ€§è¯·å‚è€ƒ[openMind å¾®è°ƒæ–‡æ¡£](https://modelers.cn/docs/zh/openmind-library/1.0.0/basic_tutorial/finetune/finetune_pt.html)ã€‚

### æ¨ç†

é™¤äº†å¾®è°ƒä»¥å¤–ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ openMind Library è¿›è¡Œæ¨¡å‹æ¨ç†ï¼Œå®‰è£… openMind Library åï¼Œä½¿ç”¨
ä¸‹è¿°å‘½ä»¤è¡Œå³å¯è¿›è¡Œå•è½®æ¨ç†ï¼š

```shell
openmind-cli run Intern/internlm3-8b-instruct --task text-generation --input '{"text_inputs":"What is AI?","max_length":512}' --trust_remote_code 1
```

æ›´å¤šç‰¹æ€§è¯·å‚è€ƒ[openMind æ¨ç†æ–‡æ¡£](https://modelers.cn/docs/zh/openmind-library/1.0.0/basic_tutorial/pipeline.html)ã€‚

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç å’Œæƒé‡ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚